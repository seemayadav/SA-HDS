# SA-HDS: Speaker-Aware Hybrid Dialogue Summarization

This repository contains the implementation of **SA-HDS**, a speaker-aware hybrid dialogue summarization framework that integrates **multi-objective extractive summarization using the Honey Badger Algorithm (MO-HBA)** with **prompt-based abstractive summarization using Large Language Models (LLMs)**.

The framework is designed to handle **multi-speaker, long-form, and query-focused dialogues**, such as meetings, interviews, and conversational transcripts, while explicitly optimizing multiple objectives like informativeness, speaker diversity, redundancy reduction, and query relevance.

---

## ğŸ” Key Features

- **Speaker-Aware Modeling**  
  Explicitly models speaker identity to ensure balanced representation across participants.

- **Multi-Objective Optimization**  
  Uses a **Multi-Objective Honey Badger Algorithm (MO-HBA)** to optimize:
  - Informativeness
  - Query relevance
  - Speaker diversity
  - Redundancy minimization
  - Summary length constraint

- **Hybrid Summarization Pipeline**
  - **Extractive Phase:** MO-HBA selects optimal utterance subsets
  - **Abstractive Phase:** LLM refines extracted content for coherence and readability

- **Query-Focused Summarization**
  Supports user-intent driven summaries, especially effective for meeting datasets like QMSum.

- **Robust Evaluation**
  Evaluated using **ROUGE, BLEU, and BERTScore** metrics.

---

## ğŸ§  Methodology Overview

The SA-HDS framework consists of three main phases:

### 1ï¸âƒ£ Domain-Specific Phrase Extraction
- Dialogues are segmented into speaker-tagged utterances.
- TF-IDF is used to identify domain- and query-relevant keywords.
- Utterances containing important phrases are retained for optimization.

### 2ï¸âƒ£ Extractive Summarization using MO-HBA
- Candidate summaries are represented as subsets of utterances.
- MO-HBA explores and exploits the search space using:
  - **Honey search behavior (exploration)**
  - **Digging behavior (exploitation)**
- A Pareto front is maintained to balance conflicting objectives.

### 3ï¸âƒ£ Abstractive Refinement using LLM
- The best extractive summary from the Pareto archive is passed to an LLM.
- Prompt-based abstraction improves fluency, coherence, and readability while preserving factual content.

---

## ğŸ“Š Datasets Used

The framework is evaluated on three standard dialogue summarization datasets:

- **DialogSum** â€“ Real-life multi-turn dialogues with summaries and queries  
- **SAMSum** â€“ Short, informal chat-style conversations  
- **QMSum** â€“ Query-based long meeting transcripts with speaker annotations  

---

## ğŸ“ˆ Evaluation Metrics

- **ROUGE-1, ROUGE-2, ROUGE-L**
- **BLEU**
- **BERTScore**

---

## âš™ï¸ Experimental Settings

- Population Size (MO-HBA): 20  
- Iterations: 20  
- Maximum Summary Length: 20 sentences  
- Sentence Embeddings: `all-MiniLM-L6-v2`  
- Similarity Measure: Cosine similarity  

LLMs used in the abstractive phase:
- GPT-4o
- Gemma2-9B-it
- DeepSeek
- LLaMA 3
- T5

---

## ğŸ† Key Results

- MO-HBA consistently outperforms PSO and GA across all datasets.
- **HBA + GPT-4o** achieves the best performance overall.
- Significant improvements observed on **QMSum**, highlighting the frameworkâ€™s strength for long, structured, query-focused dialogues.
- Low standard deviation across runs indicates strong robustness and stability.

---

## ğŸ“ Repository Structure (Suggested)
SA-HDS/
â”‚
â”œâ”€â”€ data/ # Dataset loaders and preprocessing scripts
â”œâ”€â”€ embeddings/ # Sentence embedding utilities
â”œâ”€â”€ optimizer/
â”‚ â”œâ”€â”€ hba.py # Honey Badger Algorithm implementation
â”‚ â”œâ”€â”€ pso.py # Particle Swarm Optimization (baseline)
â”‚ â””â”€â”€ ga.py # Genetic Algorithm (baseline)
â”‚
â”œâ”€â”€ extraction/
â”‚ â””â”€â”€ mo_hba_extractive.py
â”‚
â”œâ”€â”€ abstraction/
â”‚ â””â”€â”€ llm_prompting.py
â”‚
â”œâ”€â”€ evaluation/
â”‚ â”œâ”€â”€ rouge_eval.py
â”‚ â”œâ”€â”€ bleu_eval.py
â”‚ â””â”€â”€ bertscore_eval.py
â”‚
â”œâ”€â”€ configs/
â”‚ â””â”€â”€ config.yaml
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


